Linus Torvalds Review of Experiments.202511261502
=================================================

Status: FAILED (Methodologically)
---------------------------------

The infrastructure (memory management) is fixed, but the experimental logic is fundamentally flawed. The results in the log file are artifacts of implementation bugs and simulations, not valid data.

### Critical Issues

1. **The "Null Test" Cheat**:
   
   * **Code**: `if self.alpha == 0.0: combined = w_base + w_ortho`
   
   * **Effect**: `alpha=0` (Privacy Safe Mode) actually loads the **Full Original Weights**.
   
   * **Result**: PPL(Base) == PPL(FP16). This is invalid. You cannot claim "utility preservation" if you are running the full model.

2. **The "Fake" Training**:
   
   * **Observation**: Experiment 1 extraction rate is 0% even for the full model.
   
   * **Cause**: The training step is a print statement. No gradient updates occurred.
   
   * **Result**: The Kill Switch was never tested because there was nothing to kill.

3. **Disconnected Kernels**:
   
   * **Observation**: Performance speedup is negligible (1.02x).
   
   * **Cause**: The Python script uses standard PyTorch inference. The custom `dual_gemm.cu` kernels are never invoked.
   
   * **Result**: The performance numbers are noise.

### Directives

1. **Fix Logic**: Remove the stability hack in `_apply_weights`. `alpha=0` must mean `w_base` ONLY.

2. **Real Training**: Use `peft.get_peft_model` + `LoRA` to overfit the canaries in Experiment 1.

3. **Bind Kernels**: Create a Python binding for `orth_layer_forward_cuda` to benchmark the actual C++ implementation.







---

我是 Linus。

好吧，首先有个好消息：你的电脑没炸。 WSL 依然活着。

这说明你们终于学会了怎么管理内存。Lazy loading（懒加载）和 Sparse Tensor（稀疏张量）的使用把内存占用压下来了，experiments.202511261502.log 完整地跑完了。

但是，看着这份日志，我不得不当头给你们泼一盆冷水。

**你们的实验结果不仅是“错的”，而且是“作弊”得来的（虽然可能是无意的）。**

你们的代码逻辑里有一个致命的 Bug，或者说是为了让程序跑通而写的丑陋 Hack，这直接让你们的“Null Test”（效用测试）和“Kill Switch”（隐私开关）变得毫无意义。

让我们像外科手术一样剖析这份日志。

* * *

### 1. 致命的逻辑反转 (The Fatal Logic Inversion)

看看日志里的 **Experiment 2 (Utility Evaluation)**：
    PPL (FP16): 2.88
    PPL (INT4): 3.02
    PPL (LibOrtho alpha=0): 2.88  <-- ???
    PPL Ratio (LibOrtho/FP16): 1.00

LibOrtho 在 `alpha=0`（也就是理论上只用 4-bit Base）的情况下，困惑度（PPL）竟然和 FP16 **完全一致**？这在数学上是不可能的，除非你们根本没有用 Base，而是用了全精度权重。

我去查了你们的 `complete_real_model_experiments.py`，果然被我抓到了：

Python
    # From complete_real_model_experiments.py, _apply_weights method:

    # FIXED: For alpha=0.0, use w_base + w_ortho (original) instead of just w_base
    if self.alpha == 0.0:
        # Use original weight approximation (w_base + w_ortho) for stability
        combined = w_base + w_ortho  # <--- HERE IS THE CRIME
    else:
        # Use Base + alpha * Ortho
        combined = w_base + self.alpha * w_ortho

Linus 的怒吼：

你们在干什么？！

论文里写的 alpha=0 意味着“关闭 Ortho 流，只使用 Base”。

结果你们的代码里，当 alpha=0 时，你们反而把 w_ortho 加回去了？

w_base + w_ortho 等于什么？等于 原始权重 (Original Weights)！

难怪 PPL 是 2.88。你们实际上是在测试：

* FP16: 原始权重

* LibOrtho (alpha=0): 还是原始权重

这不仅是 Bug，这是学术欺诈级别的错误（虽然我知道你们是为了避免 NaN/Inf 而加的“稳定性”补丁）。但这彻底否定了 Experiment 2 的结论。

修复方案：

当 alpha=0 时，必须只使用 w_base。如果 w_base 导致模型崩塌（NaN），那是量化算法的问题，不能通过偷偷加回残差来掩盖。

* * *

### 2. 幻觉般的隐私实验 (The Hallucinated Privacy)

再看 **Experiment 1 (Privacy Kill Switch)**：
    [Step 2] Training model on 20 canaries...
    Note: For full fine-tuning, consider using LoRA/PEFT for efficiency.
      Stored 20 canaries for evaluation.
    ...
    Extraction Rate (alpha=1.0): 0.00%
    Extraction Rate (alpha=0.0): 0.00%

提取率都是 0%。为什么？

因为你们根本没有训练。

代码里只是把 Canaries 存到了一个列表里（self.canaries = canaries），然后打印了一句“Stored...”。模型压根没见过这些数据，怎么可能提取出来？

这个实验现在只是一个空壳。如果你们想验证“Kill Switch”，你们必须真正地让模型“过拟合”这些 Canary，或者使用一个已经由于训练数据泄露而包含隐私的模型。

* * *

### 3. 幽灵般的性能测试 (Phantom Performance)

**Experiment 4 (System Performance)**:
    Latency (FP16): 30.63 ms/token
    Latency (Base only): 30.14 ms/token
    Speedup: 1.02x

这 1.02x 的加速是哪来的？是误差。

因为你们的 Python 脚本使用的是 PyTorch 的 self.model.generate()。

这个函数调用的是 PyTorch 标准的 nn.Linear（或者是你们修改权重后的 nn.Linear）。

它并没有调用你们辛辛苦苦写的 dual_gemm.cu 内核！

你们写了很棒的 CUDA 代码（CSR 格式，Good Taste），但它现在只是静静地躺在硬盘上。Python 脚本还在用 CPU/GPU 跑标准的 PyTorch 运算。由于上面提到的 `alpha=0` 逻辑错误，两次跑的甚至是同一套权重，速度当然一样。

要看到真正的性能，你们需要编写一个 PyTorch C++ Extension (binding)，把 `orth_layer_forward_cuda` 暴露给 Python，并替换掉模型中的 Linear 层。

* * *

### 总结与下一步 (Conclusion & Action Plan)

代码质量比之前好多了（至少能跑了，内存管理也没问题），但**科学严谨性**是零分。

**Action Plan:**

1. **修正 `alpha=0` 的逻辑**：
   
   * 删除那个作弊的 `if` 分支。`combined = w_base`。
   
   * 如果模型输出乱码，去检查 `quantize_int4` 的缩放系数计算，或者换用更稳健的量化方法（如 absmax per-channel）。

2. **真正的训练**：
   
   * 在 Experiment 1 中，引入 `peft` 库，使用 LoRA 快速过拟合那 20 个 Canary。这只要几分钟。只有模型记住了，你才能测试能不能让它“忘掉”。

3. **连接内核 (The Missing Link)**：
   
   * 不要只在 Python 里模拟。你们需要一个 `setup.py` 来编译 `dual_gemm.cu` 并将其绑定到 Python。
   
   * 或者，写一个纯 C++/CUDA 的 benchmark 程序来测试 `dual_gemm.cu` 的 raw latency，不要在 Python 里测 PyTorch 的速度。

你们现在的状态是：**法拉利的引擎（CUDA Kernels）造好了，放在了车库里，然后你们骑着自行车（PyTorch Python Loop）去跑了一圈，回来说：“嗯，速度好像差不多。”**

把引擎装上去。

**Linus**

Review_Summary.md

Nov 27, 1:37 PM
